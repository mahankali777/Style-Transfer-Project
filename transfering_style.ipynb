{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahankali777/Style-Transfer-Project/blob/main/transfering_style.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "432ba4b0",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2022-02-15T09:35:18.042362Z",
          "iopub.status.busy": "2022-02-15T09:35:18.041729Z",
          "iopub.status.idle": "2022-02-15T09:35:31.079564Z",
          "shell.execute_reply": "2022-02-15T09:35:31.064753Z",
          "shell.execute_reply.started": "2021-12-03T09:29:21.000246Z"
        },
        "papermill": {
          "duration": 13.137401,
          "end_time": "2022-02-15T09:35:31.079780",
          "exception": false,
          "start_time": "2022-02-15T09:35:17.942379",
          "status": "completed"
        },
        "tags": [],
        "id": "432ba4b0"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3311ef6a",
      "metadata": {
        "papermill": {
          "duration": 0.049851,
          "end_time": "2022-02-15T09:35:31.196774",
          "exception": false,
          "start_time": "2022-02-15T09:35:31.146923",
          "status": "completed"
        },
        "tags": [],
        "id": "3311ef6a"
      },
      "source": [
        "---------------------------------------\n",
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4295364c",
      "metadata": {
        "papermill": {
          "duration": 0.050483,
          "end_time": "2022-02-15T09:35:31.298842",
          "exception": false,
          "start_time": "2022-02-15T09:35:31.248359",
          "status": "completed"
        },
        "tags": [],
        "id": "4295364c"
      },
      "source": [
        "![](https://miro.medium.com/max/1200/1*XI3beonBnOwp-y5BwNOqCw.gif)\n",
        "\n",
        "Picture Credit: https://miro.medium.com"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e952c45",
      "metadata": {
        "papermill": {
          "duration": 0.050779,
          "end_time": "2022-02-15T09:35:31.400185",
          "exception": false,
          "start_time": "2022-02-15T09:35:31.349406",
          "status": "completed"
        },
        "tags": [],
        "id": "1e952c45"
      },
      "source": [
        "**Neural Style Transfer**\n",
        "\n",
        "> Neural Style Transfer (NST) refers to a class of software algorithms that manipulate digital images, or videos, in order to adopt the appearance or visual style of another image. NST algorithms are characterized by their use of deep neural networks for the sake of image transformation. Common uses for NST are the creation of artificial artwork from photographs, for example by transferring the appearance of famous paintings to user-supplied photographs. Several notable mobile apps use NST techniques for this purpose, including DeepArt and Prisma. This method has been used by artists and designers around the globe to develop new artwork based on existent style(s).\n",
        "\n",
        "Ref: https://en.wikipedia.org/wiki/Neural_Style_Transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "591e84eb",
      "metadata": {
        "papermill": {
          "duration": 0.049476,
          "end_time": "2022-02-15T09:35:31.502131",
          "exception": false,
          "start_time": "2022-02-15T09:35:31.452655",
          "status": "completed"
        },
        "tags": [],
        "id": "591e84eb"
      },
      "source": [
        "Style transfer means that when a content image and a style image are given, the outline and shape of the image are similar to the content image, and the color or texture is changed to be similar to the style image.\n",
        "\n",
        "By separating content and style, you can mix content and style of different images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd8e79f8",
      "metadata": {
        "papermill": {
          "duration": 0.049089,
          "end_time": "2022-02-15T09:35:31.600986",
          "exception": false,
          "start_time": "2022-02-15T09:35:31.551897",
          "status": "completed"
        },
        "tags": [],
        "id": "bd8e79f8"
      },
      "source": [
        "A pre-trained VGG19 Net is used as a model to extract content and style. It then uses the losses of the content and style to iteratively update the target image until the desired result is achieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "b4e84b1b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-15T09:35:31.706834Z",
          "iopub.status.busy": "2022-02-15T09:35:31.706287Z",
          "iopub.status.idle": "2022-02-15T09:35:36.178067Z",
          "shell.execute_reply": "2022-02-15T09:35:36.177260Z",
          "shell.execute_reply.started": "2021-12-03T09:29:34.153146Z"
        },
        "papermill": {
          "duration": 4.527866,
          "end_time": "2022-02-15T09:35:36.178231",
          "exception": false,
          "start_time": "2022-02-15T09:35:31.650365",
          "status": "completed"
        },
        "tags": [],
        "id": "b4e84b1b"
      },
      "outputs": [],
      "source": [
        "# import resources\n",
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import requests\n",
        "from torchvision import transforms, models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce5ecca6",
      "metadata": {
        "papermill": {
          "duration": 0.049735,
          "end_time": "2022-02-15T09:35:36.278637",
          "exception": false,
          "start_time": "2022-02-15T09:35:36.228902",
          "status": "completed"
        },
        "tags": [],
        "id": "ce5ecca6"
      },
      "source": [
        "------------------------------------------------\n",
        "# 1. Load in model\n",
        "\n",
        "VGG19 is divided into two parts.\n",
        "\n",
        "* vgg19.features: All convolutional layers and pooling layers\n",
        "* vgg19.classifier: The last three linear layers are the classifier layer.\n",
        "\n",
        "We only need the features part. And \"freeze\" so that the weight is not updated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "8c6adb81",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-15T09:35:36.383993Z",
          "iopub.status.busy": "2022-02-15T09:35:36.383456Z",
          "iopub.status.idle": "2022-02-15T09:35:57.515916Z",
          "shell.execute_reply": "2022-02-15T09:35:57.515421Z",
          "shell.execute_reply.started": "2021-12-03T09:29:35.414421Z"
        },
        "papermill": {
          "duration": 21.187586,
          "end_time": "2022-02-15T09:35:57.516040",
          "exception": false,
          "start_time": "2022-02-15T09:35:36.328454",
          "status": "completed"
        },
        "tags": [],
        "id": "8c6adb81"
      },
      "outputs": [],
      "source": [
        "# get the \"features\" portion of VGG19 (we will not need the \"classifier\" portion)\n",
        "vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "# freeze all VGG parameters since we're only optimizing the target image\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e14d1cc5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-15T09:35:57.660694Z",
          "iopub.status.busy": "2022-02-15T09:35:57.660084Z",
          "iopub.status.idle": "2022-02-15T09:36:02.794668Z",
          "shell.execute_reply": "2022-02-15T09:36:02.793943Z",
          "shell.execute_reply.started": "2021-12-03T09:29:40.19008Z"
        },
        "papermill": {
          "duration": 5.227876,
          "end_time": "2022-02-15T09:36:02.794803",
          "exception": false,
          "start_time": "2022-02-15T09:35:57.566927",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e14d1cc5",
        "outputId": "a422a810-0fd8-43ae-c9e1-8b09ef115101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:09<00:00, 62.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# get the \"features\" portion of VGG19 (we will not need the \"classifier\" portion)\n",
        "vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "# freeze all VGG parameters since we're only optimizing the target image\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b430d9f8",
      "metadata": {
        "papermill": {
          "duration": 0.052727,
          "end_time": "2022-02-15T09:36:02.898466",
          "exception": false,
          "start_time": "2022-02-15T09:36:02.845739",
          "status": "completed"
        },
        "tags": [],
        "id": "b430d9f8"
      },
      "source": [
        "-----------------------------------------------\n",
        "# 2. Load in Content and Style Images\n",
        "\n",
        "Load the content image and style image to be used for style transfer. The load_image function transforms the image and loads it in the form of normalized Tensors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "2025b645",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-15T09:36:03.011215Z",
          "iopub.status.busy": "2022-02-15T09:36:03.010272Z",
          "iopub.status.idle": "2022-02-15T09:36:03.012177Z",
          "shell.execute_reply": "2022-02-15T09:36:03.012620Z",
          "shell.execute_reply.started": "2021-12-03T09:29:40.204847Z"
        },
        "papermill": {
          "duration": 0.062897,
          "end_time": "2022-02-15T09:36:03.012758",
          "exception": false,
          "start_time": "2022-02-15T09:36:02.949861",
          "status": "completed"
        },
        "tags": [],
        "id": "2025b645"
      },
      "outputs": [],
      "source": [
        "def load_image(img_path, max_size=128, shape=None):\n",
        "    ''' Load in and transform an image, making sure the image\n",
        "       is <= 400 pixels in the x-y dims.'''\n",
        "    if \"http\" in img_path:\n",
        "        response = requests.get(img_path)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # large images will slow down processing\n",
        "    if max(image.size) > max_size:\n",
        "        size = max_size\n",
        "    else:\n",
        "        size = max(image.size)\n",
        "\n",
        "    if shape is not None:\n",
        "        size = shape\n",
        "\n",
        "    in_transform = transforms.Compose([\n",
        "                        transforms.Resize(size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                                             (0.229, 0.224, 0.225))])\n",
        "\n",
        "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
        "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a02467a",
      "metadata": {
        "papermill": {
          "duration": 0.04901,
          "end_time": "2022-02-15T09:36:03.111834",
          "exception": false,
          "start_time": "2022-02-15T09:36:03.062824",
          "status": "completed"
        },
        "tags": [],
        "id": "3a02467a"
      },
      "source": [
        "Load the content image and style image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3d3c398f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-15T09:36:03.215063Z",
          "iopub.status.busy": "2022-02-15T09:36:03.214471Z",
          "iopub.status.idle": "2022-02-15T09:36:03.447019Z",
          "shell.execute_reply": "2022-02-15T09:36:03.445977Z",
          "shell.execute_reply.started": "2021-12-03T09:29:40.451488Z"
        },
        "papermill": {
          "duration": 0.286306,
          "end_time": "2022-02-15T09:36:03.447192",
          "exception": false,
          "start_time": "2022-02-15T09:36:03.160886",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d3c398f",
        "outputId": "adde01c6-23d3-4881-e41d-5630326be152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/rithishkannas/stylized-image-dataset/versions/3\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rithishkannas/stylized-image-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "079dbf91",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-15T09:36:03.554751Z",
          "iopub.status.busy": "2022-02-15T09:36:03.554180Z",
          "iopub.status.idle": "2022-02-15T09:36:03.557031Z",
          "shell.execute_reply": "2022-02-15T09:36:03.556622Z",
          "shell.execute_reply.started": "2021-12-03T09:29:40.717501Z"
        },
        "papermill": {
          "duration": 0.058866,
          "end_time": "2022-02-15T09:36:03.557169",
          "exception": false,
          "start_time": "2022-02-15T09:36:03.498303",
          "status": "completed"
        },
        "tags": [],
        "id": "079dbf91"
      },
      "outputs": [],
      "source": [
        "# helper function for un-normalizing an image\n",
        "# and converting it from a Tensor image to a NumPy image for display\n",
        "def im_convert(tensor):\n",
        "    \"\"\" Display a tensor as an image. \"\"\"\n",
        "\n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.numpy().squeeze()\n",
        "    image = image.transpose(1,2,0)\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "98ac129f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-15T09:36:03.660528Z",
          "iopub.status.busy": "2022-02-15T09:36:03.659742Z",
          "iopub.status.idle": "2022-02-15T09:36:04.177316Z",
          "shell.execute_reply": "2022-02-15T09:36:04.177729Z",
          "shell.execute_reply.started": "2021-12-03T09:29:40.726173Z"
        },
        "papermill": {
          "duration": 0.572122,
          "end_time": "2022-02-15T09:36:04.177871",
          "exception": false,
          "start_time": "2022-02-15T09:36:03.605749",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98ac129f",
        "outputId": "bedcaaea-bdb8-47bf-a615-0dfab14c2143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/vbookshelf/art-by-ai-neural-style-transfer?dataset_version_number=5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 463M/463M [00:06<00:00, 72.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/vbookshelf/art-by-ai-neural-style-transfer/versions/5\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"vbookshelf/art-by-ai-neural-style-transfer\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a06e2504",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-15T09:36:04.300134Z",
          "iopub.status.busy": "2022-02-15T09:36:04.299429Z",
          "iopub.status.idle": "2022-02-15T09:36:04.302462Z",
          "shell.execute_reply": "2022-02-15T09:36:04.301998Z",
          "shell.execute_reply.started": "2021-12-03T09:29:41.347137Z"
        },
        "papermill": {
          "duration": 0.066672,
          "end_time": "2022-02-15T09:36:04.302575",
          "exception": false,
          "start_time": "2022-02-15T09:36:04.235903",
          "status": "completed"
        },
        "tags": [],
        "id": "a06e2504"
      },
      "outputs": [],
      "source": [
        "def get_features(image, model, layers=None):\n",
        "    \"\"\" Run an image forward through a model and get the features for\n",
        "        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n",
        "    \"\"\"\n",
        "\n",
        "    if layers is None:\n",
        "        layers = {'0': 'conv1_1',\n",
        "                  '5': 'conv2_1',\n",
        "                  '10': 'conv3_1',\n",
        "                  '19': 'conv4_1',\n",
        "                  '21': 'conv4_2',  ## content representation\n",
        "                  '28': 'conv5_1'}\n",
        "\n",
        "    features = {}\n",
        "    x = image\n",
        "    # model._modules is a dictionary holding each module in the model\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d6f88cf",
      "metadata": {
        "papermill": {
          "duration": 0.058466,
          "end_time": "2022-02-15T09:36:04.417945",
          "exception": false,
          "start_time": "2022-02-15T09:36:04.359479",
          "status": "completed"
        },
        "tags": [],
        "id": "0d6f88cf"
      },
      "source": [
        "--------------------------------------------------\n",
        "# 3. Gram Matrix\n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*VAQs1KSfbysnloPah_fHGQ.gif)\n",
        "\n",
        "Picture Credit: https://miro.medium.com\n",
        "\n",
        "\n",
        "> The matrix expressing the correlation of this Channel is called Gram Matrix. Loss is minimized by defining the difference between this Gram Matrix and the Gram Matrix of the newly created image as a Loss Function. Next, in order to reflect the content, the loss function is calculated in units of pixels from the feature map spit out from each pre-trained CNN. In this way, a new image is created that minimizes the Loss calculated from Style and Loss calculated from Content.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Gram_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1d9421fb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-15T09:36:04.597619Z",
          "iopub.status.busy": "2022-02-15T09:36:04.596693Z",
          "iopub.status.idle": "2022-02-15T09:36:04.598514Z",
          "shell.execute_reply": "2022-02-15T09:36:04.599041Z",
          "shell.execute_reply.started": "2021-12-03T09:29:41.355742Z"
        },
        "papermill": {
          "duration": 0.114623,
          "end_time": "2022-02-15T09:36:04.599214",
          "exception": false,
          "start_time": "2022-02-15T09:36:04.484591",
          "status": "completed"
        },
        "tags": [],
        "id": "1d9421fb"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(tensor):\n",
        "    \"\"\" Calculate the Gram Matrix of a given tensor\n",
        "    \"\"\"\n",
        "\n",
        "    # get the batch_size, depth, height, and width of the Tensor\n",
        "    _, d, h, w = tensor.size()\n",
        "\n",
        "    # reshape so we're multiplying the features for each channel\n",
        "    tensor = tensor.view(d, h * w)\n",
        "\n",
        "    # calculate the gram matrix\n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "\n",
        "    return gram"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bef848e",
      "metadata": {
        "papermill": {
          "duration": 0.057825,
          "end_time": "2022-02-15T09:36:04.717744",
          "exception": false,
          "start_time": "2022-02-15T09:36:04.659919",
          "status": "completed"
        },
        "tags": [],
        "id": "5bef848e"
      },
      "source": [
        "The function that extracts the features of a given convolutional layer and computes the Gram Matrix is made. Putting it all together, we extract the features from the image and compute the Gram Matrix for each layer from the style representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4380c739",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-15T09:36:04.851111Z",
          "iopub.status.busy": "2022-02-15T09:36:04.850530Z",
          "iopub.status.idle": "2022-02-15T09:36:04.942975Z",
          "shell.execute_reply": "2022-02-15T09:36:04.943442Z",
          "shell.execute_reply.started": "2021-12-03T09:29:41.37451Z"
        },
        "papermill": {
          "duration": 0.166923,
          "end_time": "2022-02-15T09:36:04.943643",
          "exception": false,
          "start_time": "2022-02-15T09:36:04.776720",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4380c739",
        "outputId": "31677ef4-2ec9-4f2e-ef9a-72507632e16e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/burhanuddinlatsaheb/neural-style-transfer?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.17M/3.17M [00:00<00:00, 127MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/burhanuddinlatsaheb/neural-style-transfer/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"burhanuddinlatsaheb/neural-style-transfer\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0210957",
      "metadata": {
        "papermill": {
          "duration": 0.058649,
          "end_time": "2022-02-15T09:36:05.061601",
          "exception": false,
          "start_time": "2022-02-15T09:36:05.002952",
          "status": "completed"
        },
        "tags": [],
        "id": "b0210957"
      },
      "source": [
        "---------------------------------------------\n",
        "# 4. Define Losses and Weights\n",
        "\n",
        "**Individual Layer Style Weights**\n",
        "\n",
        "You can give the option to weight the style expression in each relevant layer. It is recommended that the layer weight range from 0 to 1. By giving more weight to conv1_1 and conv2_1, more style artifacts can be reflected in the final target image.\n",
        "\n",
        "**Content and Style Weight**\n",
        "\n",
        "Define alpha (content_weight) and beta (style_weight). This ratio affects the style of the final image. It is recommended to leave content_weight = 1 and set style_weight to achieve the desired ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cb93aca1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-15T09:36:05.181572Z",
          "iopub.status.busy": "2022-02-15T09:36:05.180740Z",
          "iopub.status.idle": "2022-02-15T09:36:05.182690Z",
          "shell.execute_reply": "2022-02-15T09:36:05.183105Z",
          "shell.execute_reply.started": "2021-12-03T09:29:41.779218Z"
        },
        "papermill": {
          "duration": 0.064726,
          "end_time": "2022-02-15T09:36:05.183246",
          "exception": false,
          "start_time": "2022-02-15T09:36:05.118520",
          "status": "completed"
        },
        "tags": [],
        "id": "cb93aca1"
      },
      "outputs": [],
      "source": [
        "# weights for each style layer\n",
        "# weighting earlier layers more will result in *larger* style artifacts\n",
        "# notice we are excluding `conv4_2` our content representation\n",
        "style_weights = {'conv1_1': 1,\n",
        "                 'conv2_1': 0.75,\n",
        "                 'conv3_1': 0.2,\n",
        "                 'conv4_1': 0.2,\n",
        "                 'conv5_1': 0.2}\n",
        "\n",
        "content_weight = 1  # alpha\n",
        "style_weight = 1e3  # beta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "968df8c5",
      "metadata": {
        "papermill": {
          "duration": 0.058063,
          "end_time": "2022-02-15T09:36:05.299033",
          "exception": false,
          "start_time": "2022-02-15T09:36:05.240970",
          "status": "completed"
        },
        "tags": [],
        "id": "968df8c5"
      },
      "source": [
        "# 5. Update Target and Calculate Losses\n",
        "\n",
        "**Content Loss**\n",
        "\n",
        "The content loss is calculated as the MSE between the target and the content feature in the 'conv4_2' layer.\n",
        "\n",
        "**Style Loss**\n",
        "\n",
        "The style loss is the loss between the target image and the style image. That is, it refers to the difference between the gram matrix of the style image and the gram matrix of the target image. Loss is calculated using MSE\n",
        "\n",
        "**Total Loss**\n",
        "\n",
        "Finally, the total loss is calculated by summing the style and content losses and weighting them with the specified alpha and beta."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Download the dataset (this should be done outside of the load_image function)\n",
        "# path = kagglehub.dataset_download(\"dmitryellison/egodekadem\")\n",
        "# print(\"Path to dataset files:\", path)\n",
        "\n",
        "def load_image(img_path, max_size=128, shape=None):\n",
        "    ''' Load in and transform an image, making sure the image\n",
        "       is <= 400 pixels in the x-y dims.'''\n",
        "    if \"http\" in img_path:\n",
        "        response = requests.get(img_path)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        # Replace \"path/to/your/content_image.jpg\" with the actual path to your image file.\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # large images will slow down processing\n",
        "    if max(image.size) > max_size:\n",
        "        size = max_size\n",
        "    else:\n",
        "        size = max(image.size)\n",
        "\n",
        "    if shape is not None:\n",
        "        size = shape\n",
        "\n",
        "    in_transform = transforms.Compose([\n",
        "                        transforms.Resize(size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                                             (0.229, 0.224, 0.225))])\n",
        "\n",
        "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
        "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def im_convert(tensor):\n",
        "    \"\"\" Display a tensor as an image. \"\"\"\n",
        "\n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.numpy().squeeze()\n",
        "    image = image.transpose(1,2,0)\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def get_features(image, model, layers=None):\n",
        "    \"\"\" Run an image forward through a model and get the features for\n",
        "        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n",
        "    \"\"\"\n",
        "\n",
        "    if layers is None:\n",
        "        layers = {'0': 'conv1_1',\n",
        "                  '5': 'conv2_1',\n",
        "                  '10': 'conv3_1',\n",
        "                  '19': 'conv4_1',\n",
        "                  '21': 'conv4_2',  ## content representation\n",
        "                  '28': 'conv5_1'}\n",
        "\n",
        "    features = {}\n",
        "    x = image\n",
        "    # model._modules is a dictionary holding each module in the model\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def gram_matrix(tensor):\n",
        "    \"\"\" Calculate the Gram Matrix of a given tensor\n",
        "    \"\"\"\n",
        "\n",
        "    # get the batch_size, depth, height, and width of the Tensor\n",
        "    _, d, h, w = tensor.size()\n",
        "\n",
        "    # reshape so we're multiplying the features for each channel\n",
        "    tensor = tensor.view(d, h * w)\n",
        "\n",
        "    # calculate the gram matrix\n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "\n",
        "    return gram\n",
        "\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"duttasd28/image-style-transfergoogle-images\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "hdCHYByqYRfA",
        "outputId": "b0da57c2-ae46-4a0f-f410-3ed00925f468",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hdCHYByqYRfA",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/duttasd28/image-style-transfergoogle-images/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c41ad790",
      "metadata": {
        "papermill": {
          "duration": 0.189435,
          "end_time": "2022-02-15T09:36:43.964168",
          "exception": false,
          "start_time": "2022-02-15T09:36:43.774733",
          "status": "completed"
        },
        "tags": [],
        "id": "c41ad790"
      },
      "source": [
        "-----------------------------------------------------------------\n",
        "# 6. Check the last result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a72d5c30",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-15T09:36:44.355018Z",
          "iopub.status.busy": "2022-02-15T09:36:44.353015Z",
          "iopub.status.idle": "2022-02-15T09:36:44.989031Z",
          "shell.execute_reply": "2022-02-15T09:36:44.989490Z",
          "shell.execute_reply.started": "2021-12-03T09:33:08.265651Z"
        },
        "papermill": {
          "duration": 0.834965,
          "end_time": "2022-02-15T09:36:44.989698",
          "exception": false,
          "start_time": "2022-02-15T09:36:44.154733",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a72d5c30",
        "outputId": "ee291747-b31d-48a6-a23f-5c8e67427878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/tamajitbhattacharjee/neural-style-transfer?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.04M/2.04M [00:00<00:00, 111MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/tamajitbhattacharjee/neural-style-transfer/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"tamajitbhattacharjee/neural-style-transfer\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 94.629355,
      "end_time": "2022-02-15T09:36:46.028377",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-02-15T09:35:11.399022",
      "version": "2.3.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}